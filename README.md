# Pipeline ETL avec Airflow, PySpark et PostgreSQL ğŸš€

Ce projet met en place un environnement **Apache Airflow** orchestrant un pipeline **ETL** avec **PySpark** et **PostgreSQL**. Il permet de traiter et analyser des donnÃ©es issues de capteurs de vÃ©hicules autonomes pour dÃ©tecter des pannes en temps rÃ©el.

[![Apache Airflow](https://img.shields.io/badge/Apache-Airflow-blue?logo=apache-airflow)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-blue?logo=postgresql)](https://www.postgresql.org/)
[![PySpark](https://img.shields.io/badge/PySpark-orange?logo=apachespark)](https://spark.apache.org/)

---

## **ğŸ“Œ Table des MatiÃ¨res**
- [ğŸš€ FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [ğŸ”§ PrÃ©requis](#-prÃ©requis)
- [ğŸ“¦ Installation et DÃ©marrage](#-installation-et-dÃ©marrage)
- [ğŸ’» Utilisation](#-utilisation)
- [ğŸ“Š Pipeline ETL](#-pipeline-etl)
- [âš™ï¸ CrÃ©ation et Gestion des DAGs](#-crÃ©ation-et-gestion-des-dags)
- [ğŸ“œ Licence](#-licence)

---

## **ğŸš€ FonctionnalitÃ©s**
- Orchestration avec **Apache Airflow**.
- Extraction et transformation des donnÃ©es avec **PySpark**.
- Sauvegarde des rÃ©sultats dans **PostgreSQL**.
- ExÃ©cution **dockerisÃ©e** via **Docker Compose**.
- Interface Web pour gÃ©rer les DAGs Airflow.

---

## **ğŸ”§ PrÃ©requis**
Avant de commencer, assure-toi dâ€™avoir installÃ© :
- **Docker** et **Docker Compose** ([Installation](https://docs.docker.com/get-docker/))
- **Git** ([Installation](https://git-scm.com/downloads))

---

## **ğŸ“¦ Installation et DÃ©marrage**
### **1ï¸âƒ£ Cloner le projet**
```bash
git clone https://github.com/ollosalomon/Pyspark.git
